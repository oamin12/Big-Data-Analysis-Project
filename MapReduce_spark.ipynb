{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "from pyspark.sql.types import IntegerType\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "spark_home=\"C:/Users/omar/Downloads/spark_unzipped/spark-3.5.1-bin-hadoop3\"\n",
    "#environment settings\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'\n",
    "\n",
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Script\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+\n",
      "|price_category|count|\n",
      "+--------------+-----+\n",
      "|            34|   16|\n",
      "|            28|    7|\n",
      "|            26|   13|\n",
      "|            27|   23|\n",
      "|            12|  183|\n",
      "|            22|   10|\n",
      "|             1|26061|\n",
      "|            13|  218|\n",
      "|             6| 1471|\n",
      "|            16|   74|\n",
      "|             3| 9728|\n",
      "|            20|   19|\n",
      "|             5| 3008|\n",
      "|            19|  273|\n",
      "|            15|  223|\n",
      "|             9|  812|\n",
      "|            17|  115|\n",
      "|            35|   11|\n",
      "|             4| 4563|\n",
      "|             8|  598|\n",
      "+--------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "Price category: 34, Count: 16\n",
      "Price category: 28, Count: 7\n",
      "Price category: 26, Count: 13\n",
      "Price category: 27, Count: 23\n",
      "Price category: 12, Count: 183\n",
      "Price category: 22, Count: 10\n",
      "Price category: 1, Count: 26061\n",
      "Price category: 13, Count: 218\n",
      "Price category: 6, Count: 1471\n",
      "Price category: 16, Count: 74\n",
      "Price category: 3, Count: 9728\n",
      "Price category: 20, Count: 19\n",
      "Price category: 5, Count: 3008\n",
      "Price category: 19, Count: 273\n",
      "Price category: 15, Count: 223\n",
      "Price category: 9, Count: 812\n",
      "Price category: 17, Count: 115\n",
      "Price category: 35, Count: 11\n",
      "Price category: 4, Count: 4563\n",
      "Price category: 8, Count: 598\n",
      "Price category: 23, Count: 90\n",
      "Price category: 39, Count: 14\n",
      "Price category: 7, Count: 1222\n",
      "Price category: 10, Count: 249\n",
      "Price category: 38, Count: 8\n",
      "Price category: 25, Count: 33\n",
      "Price category: 24, Count: 32\n",
      "Price category: 29, Count: 117\n",
      "Price category: 21, Count: 47\n",
      "Price category: 11, Count: 425\n",
      "Price category: 33, Count: 11\n",
      "Price category: 14, Count: 146\n",
      "Price category: 2, Count: 15767\n",
      "Price category: 30, Count: 2\n",
      "Price category: 0, Count: 8429\n",
      "Price category: 18, Count: 60\n",
      "Price category: 37, Count: 10\n",
      "Price category: 32, Count: 7\n",
      "Price category: 36, Count: 4\n",
      "Price category: 31, Count: 14\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize SparkSession\n",
    "spark2 = SparkSession.builder.appName(\"AirbnbPriceCategories\").getOrCreate()\n",
    "\n",
    "# Read dataset\n",
    "file_path = 'Dataset/Airbnb_Data1.csv'\n",
    "df = spark2.read.csv(file_path, header=True)\n",
    "\n",
    "#preprocess all the prices to make them integers by using int() function\n",
    "df = df.withColumn(\"price\", expr(\"int(price)\"))\n",
    "\n",
    "\n",
    "# Define a function to create price categories\n",
    "def create_price_category(price):\n",
    "  \"\"\"\n",
    "  This function takes a price and assigns it to a price category based on ranges.\n",
    "  \"\"\"\n",
    "  prices = [(0 + 50 * i, 50 + 50 * i) for i in range(0, 40)]\n",
    "  return [i for i in range(len(prices)) if prices[i][0] <= int(price) <= prices[i][1]][0]\n",
    "\n",
    "# Apply the map function using User Defined Function (UDF)\n",
    "udf_create_price_category = spark.udf.register(\"create_price_category\", create_price_category, IntegerType())\n",
    "df = df.withColumn(\"price_category\", udf_create_price_category(df[\"price\"]))\n",
    "\n",
    "# Reduce function (using groupBy and count)\n",
    "price_category_counts = df.groupBy(\"price_category\").count()\n",
    "\n",
    "# Print the results\n",
    "price_category_counts.show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALT 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(price_category=0, count=8429), Row(price_category=1, count=26061), Row(price_category=2, count=15767), Row(price_category=3, count=9728), Row(price_category=4, count=4563), Row(price_category=5, count=3008), Row(price_category=6, count=1471), Row(price_category=7, count=1222), Row(price_category=8, count=598), Row(price_category=9, count=812), Row(price_category=10, count=249), Row(price_category=11, count=425), Row(price_category=12, count=183), Row(price_category=13, count=218), Row(price_category=14, count=146), Row(price_category=15, count=223), Row(price_category=16, count=74), Row(price_category=17, count=115), Row(price_category=18, count=60), Row(price_category=19, count=273), Row(price_category=20, count=19), Row(price_category=21, count=47), Row(price_category=22, count=10), Row(price_category=23, count=90), Row(price_category=24, count=32), Row(price_category=25, count=33), Row(price_category=26, count=13), Row(price_category=27, count=23), Row(price_category=28, count=7), Row(price_category=29, count=117), Row(price_category=30, count=2), Row(price_category=31, count=14), Row(price_category=32, count=7), Row(price_category=33, count=11), Row(price_category=34, count=16), Row(price_category=35, count=11), Row(price_category=36, count=4), Row(price_category=37, count=10), Row(price_category=38, count=8), Row(price_category=39, count=14)]\n"
     ]
    }
   ],
   "source": [
    "# Read dataset\n",
    "file_path = 'Dataset/Airbnb_Data1.csv'\n",
    "\n",
    "dataset = spark.read.csv(file_path, header=True)\n",
    "\n",
    "#preprocess all the prices to make them integers by using int() function\n",
    "dataset = dataset.withColumn(\"price\", expr(\"int(price)\"))\n",
    "\n",
    "\n",
    "# Define the map function\n",
    "def map_function(price):\n",
    "    \n",
    "    # Define price ranges\n",
    "    prices = [(0 + 50 * i, 50 + 50 * i) for i in range(0, 40)]\n",
    "    # Iterate over price ranges\n",
    "    for i, p in enumerate(prices):\n",
    "        # Check if the price falls within the range\n",
    "        if p[0] <= int(price) <= p[1]:\n",
    "            # Return the index of the price range\n",
    "            return i\n",
    "    # If the price does not fall within any range, return -1 or handle accordingly\n",
    "    return -1\n",
    "\n",
    "# Register the UDF\n",
    "map_udf = udf(map_function, IntegerType())\n",
    "\n",
    "# Apply the map function to create a new column\n",
    "mapped_dataset = dataset.withColumn(\"price_category\", map_udf(\"price\"))\n",
    "\n",
    "# Reduce function\n",
    "def reduce_function(rows):\n",
    "    counts = [0] * 40\n",
    "    for row in rows:\n",
    "        # Check if row['price_category'] is not -1 (indicating an invalid category)\n",
    "        if row['price_category'] != -1:\n",
    "            counts[row['price_category']] += 1\n",
    "    return counts\n",
    "\n",
    "# Group data and apply reduce function\n",
    "reduced_counts = mapped_dataset.groupBy(\"price_category\").count().orderBy(\"price_category\").collect()\n",
    "\n",
    "print(reduced_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8429, 26061, 15767, 9728, 4563, 3008, 1471, 1222, 598, 812, 249, 425, 183, 218, 146, 223, 74, 115, 60, 273, 19, 47, 10, 90, 32, 33, 13, 23, 7, 117, 2, 14, 7, 11, 16, 11, 4, 10, 8, 14]\n"
     ]
    }
   ],
   "source": [
    "#get counts only from the reduced_counts\n",
    "counts = [row['count'] for row in reduced_counts]\n",
    "\n",
    "print(counts)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
