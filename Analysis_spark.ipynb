{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "# !pip install matplotlib\n",
    "# !pip install pandas\n",
    "# !pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, regexp_replace, split,exp, explode\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "spark_home=os.environ[\"spark_home\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment settings\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = spark_home\n",
    "\n",
    "# Add Spark bin and executors to PATH\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"bin\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"sbin\")\n",
    "\n",
    "# Add Spark Python libraries to PYTHONPATH\n",
    "os.environ[\"PYTHONPATH\"] = os.path.join(spark_home, \"python\") + os.pathsep + os.environ.get(\"PYTHONPATH\", \"\")\n",
    "os.environ[\"PYTHONPATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\")\n",
    "\n",
    "# Add PySpark to the system path\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"pyspark.zip\")\n",
    "os.environ[\"PATH\"] += os.pathsep + os.path.join(spark_home, \"python\", \"lib\", \"py4j-0.10.9-src.zip\")\n",
    "\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = 'jupyter'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON_OPTS'] = 'lab'\n",
    "os.environ['PYSPARK_PYTHON'] = 'python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark-Script\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "df = spark.read.parquet(\"Dataset/Airbnb_Data.parquet\")\n",
    "\n",
    "#show the data\n",
    "df.show()\n",
    "\n",
    "#show the schema\n",
    "df.printSchema()\n",
    "\n",
    "#show the count of the data\n",
    "print(\"The count of the data is: \", df.count())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for all the amenities remove the brackets and double quotes and split them by comma to get the list of amenities\n",
    "\n",
    "df = df.withColumn(\"amenities\", regexp_replace(col(\"amenities\"), \"[{}\\\"]\", \"\"))\n",
    "df = df.withColumn(\"amenities\", split(col(\"amenities\"), \",\"))\n",
    "df.show(5)\n",
    "#print first ammenity\n",
    "print(df.select(\"amenities\").first()[0])\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace the log price column with the price column\n",
    "\n",
    "df = df.withColumn(\"price\", exp(df[\"log_price\"]))\n",
    "df.show(5)\n",
    "\n",
    "newdf=df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Explode the 'amenities' array column to get individual amenities\n",
    "amenities_df = df.select(explode('amenities').alias('amenity'))\n",
    "\n",
    "# Get unique amenities\n",
    "unique_amenities_df = amenities_df.select('amenity').distinct()\n",
    "\n",
    "# Convert DataFrame to a list of rows\n",
    "unique_amenities_rows = unique_amenities_df.collect()\n",
    "\n",
    "# Convert list of rows to a set of unique amenities\n",
    "unique_amenities_set = set(row.amenity for row in unique_amenities_rows)\n",
    "\n",
    "# Display the unique amenities\n",
    "print(unique_amenities_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of unique amenities is: \", len(unique_amenities_set))\n",
    "unique_amenities_set.remove('')\n",
    "print(\"The number of unique amenities is: \", len(unique_amenities_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique integer values of prices in the DataFrame\n",
    "prices_df = df.select('price').distinct()\n",
    "\n",
    "# Convert DataFrame to a list of rows\n",
    "prices_rows = prices_df.collect()\n",
    "\n",
    "# Extract unique integer values of prices\n",
    "prices = set(int(row.price) for row in prices_rows)\n",
    "\n",
    "# Display the unique prices\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The number of unique prices is: \", len(prices))\n",
    "prices = [(0 + 50 * i, 50 + 50 * i) for i in range(0, 40)]\n",
    "\n",
    "print(prices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique cities\n",
    "unique_city_df = df.select('city').distinct()\n",
    "\n",
    "# Convert DataFrame to a list of cities\n",
    "unique_cities = [row.city for row in unique_city_df.collect()]\n",
    "\n",
    "price_categories = [i for i in range(0, 41)]\n",
    "\n",
    "print(price_categories)\n",
    "print(unique_cities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #for each city we can see the distribution of the price categories\n",
    "# for city in unique_city:\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     sns.countplot(x='city', hue=df['price_category'], data=df[df['city'] == city])\n",
    "#     plt.title('Price Category vs City')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert Spark DataFrame to Pandas DataFrame\n",
    "# pandas_df = df.toPandas()\n",
    "\n",
    "# # Get unique cities\n",
    "# unique_city = df.select('city').distinct()\n",
    "\n",
    "# # For each city, we can see the distribution of the price categories\n",
    "# for city in unique_city:\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     sns.countplot(x='city', hue=pandas_df['price_category'], data=pandas_df[pandas_df['city'] == city])\n",
    "#     plt.title('Price Category vs City')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming that df is your Spark DataFrame\n",
    "# unique_city = df.select('city').distinct().rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "# for city in unique_city:\n",
    "#     # Filter the data for the current city and count the price categories\n",
    "#     city_data = df.filter(df['city'] == city).groupBy('city', 'price_category').count().orderBy('count', ascending=False)\n",
    "    \n",
    "#     # Convert the result to a Pandas DataFrame for plotting\n",
    "#     city_data_pd = city_data.toPandas()\n",
    "    \n",
    "#     # Plot the data\n",
    "#     plt.figure(figsize=(20,10))\n",
    "#     plt.bar(city_data_pd['price_category'], city_data_pd['count'])\n",
    "#     plt.title(f'Price Category vs City for {city}')\n",
    "#     plt.xlabel('Price Category')\n",
    "#     plt.ylabel('Count')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "csv_file_path = \"Dataset/Airbnb_Data.csv\" \n",
    "df = pd.read_csv(csv_file_path)\n",
    "\n",
    "table = pa.Table.from_pandas(df)\n",
    "\n",
    "parquet_file_path = \"Dataset/Airbnb_Data.parquet\"           \n",
    "pq.write_table(table, parquet_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
